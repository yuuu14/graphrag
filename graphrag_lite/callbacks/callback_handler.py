
from typing import Any
import logging
from collections.abc import AsyncGenerator
from pydantic import Field
from langchain_core.runnables import RunnableConfig, RunnableSerializable
from langchain_core.runnables.utils import Input, Output
from langchain_core.callbacks import AsyncCallbackHandler, AsyncCallbackManager

from graphrag_lite.utils.progress import ProgressHolder


logger = logging.getLogger(__name__)

class AsyncOperationCallbackHandler(AsyncCallbackHandler):
    async def on_chain_start(
        self,
        operation_id: str,
        inputs: dict[str, Any],
        **kwargs: Any,
    ) -> None:
        """å¼‚æ­¥å¤„ç†æ“ä½œå¼€å§‹"""
        msg = f"ğŸš€ Operation {operation_id} started with inputs: {inputs}"
        logger.info(msg)
        print(msg)

    async def on_chain_progress(
        self,
        operation_id: str,
        chain_id: str,
        progress: ProgressHolder | None = None,
        message: str | None = None,
        **kwargs: Any,
    ) -> None:
        """å¼‚æ­¥å¤„ç†è¿›åº¦æ›´æ–°"""
        if progress:
            progress.processed += 1
            progress.progress = (progress.processed / progress.total) * 100
            msg = f"ğŸ“ˆ å½“å‰æ“ä½œä¸º {operation_id}, Chain: {chain_id}, è¿›åº¦: {progress.progress:.2f}% - {message}"
            logger.info(msg)
            print(msg)
        else:
            logger.warning("No ProgressHolder set.")

    async def on_chain_error(
        self,
        operation_id: str,
        error: BaseException,  # try to trace line in operation
        **kwargs: Any,
    ) -> None:
        """logé”™è¯¯"""
        msg = f"ğŸ”¥!!! Error in {operation_id}: {error!s}"
        logger.error(msg)
        print(msg)
        # write to log file
        

    async def on_chain_end(
        self,
        operation_id: str,
        outputs: dict[str, Any],
        **kwargs: Any
    ) -> None:
        msg = f"âœ… Operation {operation_id} å®Œæˆ. Outputs: {outputs}"
        logger.info(msg)
        print(msg)

    

    
    # asyncio.gather å¤„ç†å¤šä¸ªchunk

# LLM Performance Callback
from langchain.callbacks.base import BaseCallbackHandler
import time
import os
import tiktoken

tiktoken_cache_dir = "/home/xiaoyu/tiktoken_cache"
os.environ["TIKTOKEN_CACHE_DIR"] = tiktoken_cache_dir

# validate
assert os.path.exists(os.path.join(tiktoken_cache_dir,"9b5ad71b2ce5302211f9c61530b329a4922fc6a4"))

encoding_fn = tiktoken.get_encoding("cl100k_base")


class PerformanceCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        self.start_time = None
        self.first_token_time = None
        self.end_time = None
        self.generated_tokens = 0

    # è®°å½•LLMå¼€å§‹ç”Ÿæˆçš„æ—¶é—´
    def on_llm_start(self, serialized, prompts, **kwargs):
        self.start_time = time.time()

    # æ•è·ç¬¬ä¸€ä¸ªTokençš„åˆ°è¾¾æ—¶é—´
    def on_llm_new_token(self, token, **kwargs):
        if self.first_token_time is None:
            self.first_token_time = time.time()
        self.generated_tokens += len(encoding_fn.encode(token))

    # è®°å½•ç”Ÿæˆç»“æŸæ—¶é—´å¹¶è®¡ç®—æŒ‡æ ‡
    def on_llm_end(self, response, **kwargs):
        self.end_time = time.time()
        total_time = self.end_time - self.start_time
        ttft = self.first_token_time - self.start_time if self.first_token_time else 0
        token_rate = self.generated_tokens / total_time if total_time > 0 else 0

        print(f"""
        [æ€§èƒ½æŠ¥å‘Š]
        é¦–Tokenå“åº”æ—¶é—´(TTFT): {ttft:.3f}s
        æ€»ç”Ÿæˆæ—¶é—´: {total_time:.3f}s
        ç”ŸæˆTokenæ€»æ•°: {self.generated_tokens}
        Tokenç”Ÿæˆé€Ÿç‡: {token_rate:.1f} tokens/s
        """)

